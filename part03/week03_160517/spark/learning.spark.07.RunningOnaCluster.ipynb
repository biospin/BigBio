{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07장 클러스터에서 운영하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01절 스파크 실행 구조\n",
    "\n",
    "- local 모드 \n",
    "    - 하나의 자바 프로세스에서 여러개의 쓰레드로 구동\n",
    " \n",
    "\n",
    "- 분산모드\n",
    "    - 중양조정자( 드라이버 ) + 여러개의 분산작업 노드( 익스큐터 )로 구성\n",
    "    - 드라이버와 익스큐터는 각각 독립된 자바 프로세스 \n",
    "\n",
    "![](spark01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분산모드 구성 요소 - 드라이버 \n",
    "\n",
    "- 사용자의 main 메소드가 실행되는 프로세스 \n",
    "- 주요 역할\n",
    "    - 사용자 프로그램을 테스크로 변환하여 클러스터로 전송\n",
    "    - 익스큐터에서의 개별 작업을 위한 스케쥴링을 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 드라이버의 역할\n",
    "- 사용자 프로그램을 테스트로 변환\n",
    "    - 1) 연산들의 관계를 DAG( Directed Acyclic Graph ) 생성\n",
    "    - 2) DAG를 물리적인 실행계획으로 변환\n",
    "        - 최적화를 거쳐 여러 개의 Stage로 변환\n",
    "        - 각 stage는 여러 개의 테스크를 구성\n",
    "    - 3) 단위작업들을 묶어서 클러스트로 전송\n",
    "    \n",
    "![](spark02.png)\n",
    "    \n",
    "- 익스큐터에서 태스크들의 스케쥴링\n",
    "    - 익스큐터들은 시작시 드라이버에 등록됨\n",
    "    - 드라이버는 항상 실행중인 익스큐터를 감시\n",
    "        - 테스크를 데이터 위치에 기반해 적절한 위치에서 실행이 되도록 함.\n",
    "    - 4040 포트를 사용해셔 웹 인터페이스를 실행 정보를 볼 수 있음.\n",
    "    \n",
    "    \n",
    "![](spark03.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분산모드 구성 요소 - 익스큐터\n",
    "\n",
    "- 개별 태스크를 실행하는 작업 실행 프로세스\n",
    "- 주요 역할\n",
    "    - 태스크 실행후 결과를 드라이버로 전송\n",
    "    - 사용자 프로그램에서 캐시하는 RDD를 저장하기 위한 메모리 공간 제공\n",
    "\n",
    "\n",
    "####  여러 2개의 python 코드에서  익스큐터에서 실행되는 코드를 찾아보자~~~    \n",
    "\n",
    "- 첫번째 \n",
    "```\n",
    "sc = SparkContext(master, \"WordCount\") \n",
    "lines = sc.parallelize([\"pandas\", \"i like pandas\"]) \n",
    "result = lines.flatMap(lambda x: x.split(\" \")).countByValue() \n",
    "for key, value in result.iteritems(): \n",
    "    print \"%s %i\" % (key, value) \n",
    "```\n",
    "\n",
    "- 두번째\n",
    "\n",
    "```\n",
    "sc = SparkContext(sparkMaster, appName=\"ChapterSixExample\") \n",
    "file = sc.textFile(inputFile) \n",
    "\n",
    "def validateSign(sign): \n",
    "     global validSignCount, invalidSignCount \n",
    "     if re.match(r\"\\A\\d?[a-zA-Z]{1,2}\\d{1,4}[a-zA-Z]{1,3}\\Z\", sign): \n",
    "         validSignCount += 1 \n",
    "         return True \n",
    "     else: \n",
    "         invalidSignCount += 1 \n",
    "         return False \n",
    " \n",
    " \n",
    "validSigns = callSigns.filter(validateSign) \n",
    "contactCounts = validSigns.map( \n",
    "     lambda sign: (sign, 1)).reduceByKey((lambda x, y: x + y)) \n",
    "```    \n",
    "\n",
    "\n",
    "####  MapReduce의 자바 코드에서 익스큐터에서 실행되는 코드를 찾아보자~~~    \n",
    "```\n",
    "public class WordCount {  \n",
    " \n",
    "    public static class Map extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable> {   \n",
    " \n",
    "      public void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {   \n",
    "       String line = value.toString();  \n",
    "       StringTokenizer tokenizer = new StringTokenizer(line);  \n",
    "       while (tokenizer.hasMoreTokens()) {  \n",
    "         word.set(tokenizer.nextToken());  \n",
    "         output.collect(word, one);  \n",
    "       }  \n",
    "     }  \n",
    "   }  \n",
    " \n",
    "    public static class Reduce extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> {   \n",
    "      public void reduce(Text key, Iterator<IntWritable> values, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {   \n",
    "       int sum = 0;  \n",
    "       while (values.hasNext()) {  \n",
    "         sum += values.next().get();  \n",
    "       }  \n",
    "       output.collect(key, new IntWritable(sum));  \n",
    "     }  \n",
    "   }  \n",
    " \n",
    "    public static void main(String[] args) throws Exception {   \n",
    "      JobConf conf = new JobConf(WordCount.class);   \n",
    "     conf.setJobName(\"wordcount\");  \n",
    " \n",
    "     conf.setOutputKeyClass(Text.class);  \n",
    "     conf.setOutputValueClass(IntWritable.class);  \n",
    " \n",
    "     conf.setMapperClass(Map.class);  \n",
    "     conf.setCombinerClass(Reduce.class);  \n",
    "     conf.setReducerClass(Reduce.class);  \n",
    " \n",
    "     conf.setInputFormat(TextInputFormat.class);  \n",
    "     conf.setOutputFormat(TextOutputFormat.class);  \n",
    " \n",
    "     FileInputFormat.setInputPaths(conf, new Path(args[0]));  \n",
    "     FileOutputFormat.setOutputPath(conf, new Path(args[1]));  \n",
    " \n",
    "     JobClient.runJob(conf);  \n",
    "   }  \n",
    "}  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분산모드 구성 요소 - 클러스터 매니저\n",
    "\n",
    "- 스파크는 익스큐터를 실행하기 위해 클러스터 매너저에 의존\n",
    "- 지원하는 매너저 종류\n",
    "    - standalone(내장매니저), Hadoop Yarn, Apache mesos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark-Submit \n",
    "\n",
    "- 클러스터 매니저에 스파크 어플리케이션을 제출하는 툴\n",
    "    - 예) spark-submit  --master local[*]  my_script.py\n",
    "    - 예) spark-submit  --master spark://host:7077  my_script.py\n",
    "    - 예) spark-submit  --master yarn  my_script.py\n",
    "    - 예) spark-submit  --master mesos://mesosmaster:5050  my_script.py\n",
    "\n",
    "\n",
    "- 주요옵션\n",
    "    - master : 클러스터 매니저 설정\n",
    "    - deploy-mode : 드라이버 프로그램이 실행되는 곳( client / cluster )\n",
    "    - class : 자바나 스칼라일때 main함수가 들어 있는 클래스\n",
    "    - jars/py-files : 사용자 어플리케이션에 추가돠어야 할 라이브러리 목록\n",
    "    - executor-memory : 익스큐터 프로세스가 사용할 메모리\n",
    "    - driver-memory : 드라이버 프로세스가 사용할 메모리\n",
    "    \n",
    "#### deploy-mode\n",
    "- 클라이언트 모드 \n",
    "    - 드라이버는 spark-submit의 일부로 실행됨\n",
    "    - 드라이버 프로그램을 출력을 직접 확인 가능( 표준출력 등)\n",
    "    - 어플리케이션 실행하는 동안 작업 노드들에 계속 연결되어 있어야 함.\n",
    "\n",
    "\n",
    "- 클러스터 모드\n",
    "    - 드라이버가 클러스터내의 작업 노드중에 하나에서 실행됨\n",
    "    - 실행후 개입하지 않는 방식\n",
    "    - 파이션 언어는 지원하지 않음.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02절  의존성 라이브러리의 패키징\n",
    "\n",
    "- 클러스터 머신들에 해당 라이브러리가 위치해야 함\n",
    "- 자주 사용하는 것들은 모든 머신들에 직접 설치해놓자.\n",
    "- python\n",
    "    - py-files 옵션을 사용하여 라이브러리 제출\n",
    "    \n",
    "- java, scala\n",
    "    - jars 옵션을 사용함.\n",
    "    - maven, sbt 사용함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 03절 스파크 어플리케이션 간의 스케줄링\n",
    "\n",
    "- 기본적으로 클러스터 매니저의 정책에 의존함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 클러스터 매니저\n",
    "\n",
    "- 공통\n",
    "    - 익스큐터에서 가능한 많은 코어를 써서 적은 개수 익스큐터로 어플리케이션 실행\n",
    "    - 분산된 데이터에서의 로컬리티 보장, 데이터가 있는 곳에 테스트 실행\n",
    "- Standalone\n",
    "    - 설정 : executor-memory , total-executor-cores\n",
    "    - 익스코터의 최대 개수 설정 만큼 퍼뜨리는 식으로 동작\n",
    "- Hadoop Yarn\n",
    "    - 설정 : executor-memory , executor-cores, num-excuters\n",
    "- Apache Mesos\n",
    "    - 설정 : executor-memory , total-executor-cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 클러스터 매니저 선택\n",
    "\n",
    "- Standalone\n",
    "    - 새로 배포 예정의 어플리케이션일때 <== 설정이 가장 쉬움\n",
    "- Hadoop Yarn\n",
    "    - 다른 어플리케이션과 같이 돌리거나 우수한 자원 스케쥴링이 필요한 경우\n",
    "- Apache Mesos\n",
    "    - 스파크 쉘과 같은 대화형 어플리케이션 <==  실행간 CPU 사용량을 세밀히 조정 \n",
    "- HDFS \n",
    "    - 저장소와 빠른 접근을 위해서 HDFS와 동일한 노드에 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "```\n",
    "spark-shell  --master yarn-client  --executor-memory 2G  --executor-cores 3\n",
    "\n",
    "\n",
    "import pl.elka.pw.sparkseq.seqAnalysis.SparkSeqAnalysis\n",
    "val seqAnalysis = new SparkSeqAnalysis(sc,\"file:///home/bigbio/NA18489.chrom20.ILLUMINA.bwa.YRI.exome.20121211.bam\",1,1,1)\n",
    "seqAnalysis.getCoverageBase().filter(p=>(p._2>=10)).count()\n",
    "\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
